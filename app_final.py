#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ZeroGPT Final - GerÃ§ek ChatGPT Tespiti
--------------------------------------
GerÃ§ek ChatGPT metinleriyle eÄŸitilmiÅŸ final model
"""
import streamlit as st
import joblib
import os
import re
import math
import numpy as np
import pandas as pd
from collections import Counter
from scipy.sparse import hstack

st.set_page_config(
    page_title="ZeroGPT TÃ¼rkÃ§e - Final",
    page_icon="ğŸ¯",
    layout="centered"
)

TR_STOPWORDS = {
    "ve","veya","ile","ama","fakat","ancak","Ã§Ã¼nkÃ¼","gibi","daha","Ã§ok",
    "az","bu","ÅŸu","o","bir","iki","Ã¼Ã§","her","hiÃ§","mi","mÄ±","mu","mÃ¼",
    "de","da","ki","iÃ§in","Ã¼zere","olan","olarak","sonra","Ã¶nce","ise",
    "ya","ne","neden","nasÄ±l","hangi","hem","en","ben","sen","biz","siz",
    "onlar","var","yok","diye","kadar","beri","gÃ¶re","raÄŸmen","dolayÄ±"
}

def clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = re.sub(r"http\S+|www\.\S+", " ", s)
    s = re.sub(r"[\t\r\n]+", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def extract_advanced_features(text: str) -> dict:
    features = {}
    features['text_length'] = len(text)
    features['word_count'] = len(text.split())
    
    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
    features['sentence_count'] = len(sentences)
    
    if len(sentences) > 0:
        sent_lengths = [len(s.split()) for s in sentences]
        features['avg_sentence_length'] = np.mean(sent_lengths)
        features['std_sentence_length'] = np.std(sent_lengths) if len(sent_lengths) > 1 else 0
        features['burstiness'] = features['std_sentence_length'] / (features['avg_sentence_length'] + 1e-5)
    else:
        features['avg_sentence_length'] = 0
        features['std_sentence_length'] = 0
        features['burstiness'] = 0
    
    words = re.findall(r'\w+', text.lower(), flags=re.UNICODE)
    if len(words) > 0:
        features['lexical_diversity'] = len(set(words)) / len(words)
        features['avg_word_length'] = np.mean([len(w) for w in words])
        word_counts = Counter(words)
        most_common_count = word_counts.most_common(1)[0][1]
        features['max_word_repetition'] = most_common_count / len(words)
        stopword_count = sum(1 for w in words if w in TR_STOPWORDS)
        features['stopword_ratio'] = stopword_count / len(words)
        digit_words = sum(1 for w in words if any(c.isdigit() for c in w))
        features['digit_word_ratio'] = digit_words / len(words)
    else:
        features['lexical_diversity'] = 0
        features['avg_word_length'] = 0
        features['max_word_repetition'] = 0
        features['stopword_ratio'] = 0
        features['digit_word_ratio'] = 0
    
    if len(text) > 0:
        char_counts = Counter(text.lower())
        total = sum(char_counts.values())
        probs = [c/total for c in char_counts.values()]
        features['char_entropy'] = -sum(p * math.log2(p + 1e-12) for p in probs)
    else:
        features['char_entropy'] = 0
    
    features['comma_count'] = text.count(',')
    features['question_mark_count'] = text.count('?')
    features['exclamation_count'] = text.count('!')
    features['quote_count'] = text.count('"') + text.count("'")
    
    if len(text) > 0:
        features['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / len(text)
    else:
        features['uppercase_ratio'] = 0
    
    connectors = ['sonuÃ§ olarak', 'bununla birlikte', 'diÄŸer yandan', 'Ã¶te yandan', 
                  'kÄ±sacasÄ±', 'dolayÄ±sÄ±yla', 'bu nedenle', 'ayrÄ±ca', 'bunun yanÄ±nda']
    features['connector_count'] = sum(text.lower().count(c) for c in connectors)
    
    passive_markers = ['edilmek', 'yapÄ±lmak', 'olmak', 'gÃ¶rÃ¼lmek', 'dÃ¼ÅŸÃ¼nÃ¼lmek']
    features['passive_marker_count'] = sum(text.lower().count(p) for p in passive_markers)
    
    return features

@st.cache_resource
def load_models():
    model_path = "zeroGPT_final_model.pkl"
    vec_path = "zeroGPT_final_vectorizer.pkl"
    scaler_path = "zeroGPT_final_scaler.pkl"
    
    if not all(os.path.exists(p) for p in [model_path, vec_path, scaler_path]):
        return None, None, None
    
    model = joblib.load(model_path)
    vectorizer = joblib.load(vec_path)
    scaler = joblib.load(scaler_path)
    return model, vectorizer, scaler

def predict_text(text: str, model, vectorizer, scaler):
    """Metin tahmini yap - Hata yÃ¶netimi ile korumalÄ±"""
    try:
        cleaned = clean_text(text)
        
        # Ã–zellikleri bir kez hesapla (performans iyileÅŸtirmesi)
        features = extract_advanced_features(cleaned)
        
        # TF-IDF Ã¶zellikleri
        tfidf_features = vectorizer.transform([cleaned])
        
        # Ä°statistiksel Ã¶zellikleri kullan (tekrar hesaplama yok!)
        stat_features = pd.DataFrame([features])
        stat_features_scaled = scaler.transform(stat_features)
        
        # BirleÅŸtir ve tahmin yap
        combined = hstack([tfidf_features, stat_features_scaled])
        pred = model.predict(combined)[0]
        proba = model.predict_proba(combined)[0]
        
        return pred, proba, features
        
    except MemoryError:
        st.error("âŒ **Metin Ã§ok uzun!** LÃ¼tfen daha kÄ±sa bir metin deneyin (maksimum ~5000 kelime).")
        return None, None, None
        
    except ValueError as e:
        st.error(f"âŒ **GeÃ§ersiz veri hatasÄ±:** {str(e)}")
        st.info("ğŸ’¡ LÃ¼tfen metninizin dÃ¼zgÃ¼n TÃ¼rkÃ§e karakterler iÃ§erdiÄŸinden emin olun.")
        return None, None, None
        
    except Exception as e:
        st.error(f"âŒ **Beklenmeyen hata:** {str(e)}")
        st.warning("âš ï¸ LÃ¼tfen farklÄ± bir metin deneyin veya sayfayÄ± yenileyin.")
        return None, None, None

# Ana Uygulama
st.title("ğŸ¯ ZeroGPT TÃ¼rkÃ§e - Final Versiyon")
st.markdown("### GerÃ§ek ChatGPT Metinleriyle EÄŸitilmiÅŸ AI DedektÃ¶rÃ¼")

# BaÅŸarÄ± rozetleri
col1, col2, col3 = st.columns(3)
with col1:
    st.metric("âœ… Accuracy", "99.98%")
with col2:
    st.metric("ğŸ¯ Model", "Gradient Boost")
with col3:
    st.metric("ğŸ“Š Ã–zellikler", "15K+ TF-IDF")

st.markdown("---")

model, vectorizer, scaler = load_models()

if model is None:
    st.error("âŒ Final model dosyalarÄ± bulunamadÄ±!")
    st.info("LÃ¼tfen Ã¶nce modeli eÄŸitin: `python3 retrain_with_real_chatgpt.py`")
    st.stop()

# Metin giriÅŸi
user_input = st.text_area(
    "ğŸ“ Analiz edilecek metni girin:",
    height=200,
    placeholder="Ã–rnek: Yapay zeka teknolojileri gÃ¼nÃ¼mÃ¼zde birÃ§ok sektÃ¶rde kullanÄ±lÄ±yor..."
)

if st.button("ğŸš€ Analiz Et", type="primary", use_container_width=True):
    if not user_input or len(user_input.strip()) < 20:
        st.warning("âš ï¸ LÃ¼tfen en az 20 karakter uzunluÄŸunda bir metin girin.")
    elif len(user_input.strip().split()) < 5:
        st.warning("âš ï¸ LÃ¼tfen en az 5 kelime iÃ§eren bir metin girin.")
    elif len(user_input.strip().split()) > 5000:
        st.warning("âš ï¸ Metin Ã§ok uzun! Maksimum 5000 kelime girebilirsiniz.")
    else:
        with st.spinner("ğŸ” DetaylÄ± analiz yapÄ±lÄ±yor..."):
            pred, proba, features = predict_text(user_input, model, vectorizer, scaler)
            
            # Hata kontrolÃ¼ - predict_text None dÃ¶ndÃ¼rdÃ¼yse dur
            if pred is None or proba is None or features is None:
                st.stop()  # Hata mesajÄ± zaten gÃ¶sterildi, devam etme
            
            st.markdown("---")
            
            # Ana sonuÃ§ - bÃ¼yÃ¼k ve belirgin
            if pred == 0:
                st.success("# âœ… Ä°NSAN YAZISI")
                st.metric("GÃ¼ven OranÄ±", f"%{proba[0]*100:.1f}", delta="Ä°nsan")
            else:
                st.error("# ğŸ¤– YAPAY ZEKA YAZISI")
                st.metric("GÃ¼ven OranÄ±", f"%{proba[1]*100:.1f}", delta="AI")
            
            # OlasÄ±lÄ±k Ã§ubuklarÄ±
            st.markdown("### ğŸ“Š DetaylÄ± OlasÄ±lÄ±klar")
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ğŸ‘¤ Ä°nsan**")
                st.progress(proba[0])
                st.write(f"**%{proba[0]*100:.2f}**")
            
            with col2:
                st.markdown("**ğŸ¤– Yapay Zeka**")
                st.progress(proba[1])
                st.write(f"**%{proba[1]*100:.2f}**")
            
            # Ã–zellik analizi
            st.markdown("---")
            st.markdown("### ğŸ”¬ Ã–zellik Analizi")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ğŸ“ Kelime", f"{features['word_count']:.0f}")
                st.metric("ğŸ“ CÃ¼mle", f"{features['sentence_count']:.0f}")
            
            with col2:
                st.metric("ğŸ¯ Ã‡eÅŸitlilik", f"{features['lexical_diversity']:.2f}")
                st.metric("ğŸ’¥ Burstiness", f"{features['burstiness']:.2f}")
            
            with col3:
                st.metric("ğŸ”¢ Entropi", f"{features['char_entropy']:.2f}")
                st.metric("ğŸ”— BaÄŸlaÃ§", f"{features['connector_count']:.0f}")
            
            with col4:
                st.metric("ğŸ“Œ Stopword", f"{features['stopword_ratio']:.2f}")
                st.metric("â“ Soru", f"{features['question_mark_count']:.0f}")
            
            # Yorum
            st.markdown("---")
            with st.expander("ğŸ’¡ Bu SonuÃ§ Ne Anlama Geliyor?"):
                if pred == 1:
                    st.info("""
                    **AI Tespit Ä°ÅŸaretleri:**
                    - DÃ¼zenli ve tutarlÄ± cÃ¼mle yapÄ±sÄ± (dÃ¼ÅŸÃ¼k burstiness)
                    - Formal ve akademik dil kullanÄ±mÄ±
                    - BaÄŸlaÃ§ yoÄŸunluÄŸu ("sonuÃ§ olarak", "bununla birlikte")
                    - YapÄ±landÄ±rÄ±lmÄ±ÅŸ paragraf dÃ¼zeni
                    - Tekrarlayan kalÄ±plar ve kelime seÃ§imleri
                    """)
                else:
                    st.success("""
                    **Ä°nsan YazÄ± Ä°ÅŸaretleri:**
                    - DeÄŸiÅŸken cÃ¼mle uzunluklarÄ± (yÃ¼ksek burstiness)
                    - DoÄŸal dil akÄ±ÅŸÄ± ve spontane ifadeler
                    - KiÅŸisel vurgular ve Ã¼nlemler
                    - KonuÅŸma diline yakÄ±n Ã¼slup
                    - Ã–zgÃ¼n kelime seÃ§imleri
                    """)

# Ã–rnek metinler
st.markdown("---")
with st.expander("ğŸ“š Ã–rnek Metinlerle Test Edin"):
    st.markdown("**AI Metni:**")
    st.code("""Teknoloji, insanlÄ±ÄŸÄ±n ilerlemesini hÄ±zlandÄ±ran en gÃ¼Ã§lÃ¼ araÃ§lardan biridir. 
GÃ¼nÃ¼mÃ¼zde yapay zekÃ¢, otomasyon ve bÃ¼yÃ¼k veri analizleri sayesinde Ã¼retimden 
saÄŸlÄ±ÄŸa kadar her alanda verimlilik artÄ±ÅŸÄ± saÄŸlanmaktadÄ±r.""")
    
    st.markdown("**Ä°nsan Metni:**")
    st.code("""BugÃ¼n arkadaÅŸÄ±mla sahilde yÃ¼rÃ¼dÃ¼k, hava Ã§ok gÃ¼zeldi yaa! 
Deniz kenarÄ±nda oturup sohbet ettik. AkÅŸam da gÃ¼zel bir yemek yedik.""")

# Model bilgisi
st.markdown("---")
with st.expander("â„¹ï¸ Model HakkÄ±nda"):
    st.markdown("""
    ### ğŸ“ ZeroGPT Final Model
    
    **EÄŸitim Verisi:**
    - ğŸ“Š 20,500+ TÃ¼rkÃ§e metin
    - âœ… GerÃ§ek ChatGPT Ã¶rnekleri dahil
    - ğŸ”„ Dengeli veri seti (%50 AI, %50 Ä°nsan)
    
    **Teknik Ã–zellikler:**
    - ğŸš€ Gradient Boosting Classifier (200 trees)
    - ğŸ“ˆ 15,000 TF-IDF Ã¶zellikleri (1-3 gram)
    - ğŸ”¬ 20+ istatistiksel Ã¶zellik
    - ğŸ¯ %99.98 test accuracy
    
    **Tespit Edilen Ã–zellikler:**
    - Burstiness (cÃ¼mle varyasyonu)
    - Lexical Diversity (kelime Ã§eÅŸitliliÄŸi)
    - Character Entropy (bilgi yoÄŸunluÄŸu)
    - Connector Usage (baÄŸlaÃ§ kullanÄ±mÄ±)
    - Syntactic Patterns (sÃ¶zdizimi kalÄ±plarÄ±)
    
    **Version:** 1.0 Final (GerÃ§ek ChatGPT ile eÄŸitilmiÅŸ)
    """)

st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: gray;'>ğŸ¯ ZeroGPT TÃ¼rkÃ§e Final v1.0 - GerÃ§ek ChatGPT DedektÃ¶rÃ¼</div>",
    unsafe_allow_html=True
)

